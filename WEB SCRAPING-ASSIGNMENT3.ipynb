{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469f7e10",
   "metadata": {},
   "source": [
    "# 1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e79f613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search on Amazon: i phone 14 pro max\n",
      "Search Results:\n",
      "1. Apple iPhone 14 Pro Max (256 GB) - Gold\n",
      "2. Apple iPhone 14 Pro Max (128 GB) - Gold\n",
      "3. Apple iPhone 14 Pro Max (256 GB) - Space Black\n",
      "4. Apple iPhone 14 Pro Max (512 GB) - Space Black\n",
      "5. Apple iPhone 14 Pro Max (512 GB) - Silver\n",
      "6. Apple iPhone 14 Pro Max (1 TB) - Silver\n",
      "7. Apple iPhone 14 Pro Max (256 GB) - Silver\n",
      "8. Apple iPhone 14 Pro Max (1 TB) - Space Black\n",
      "9. Nillkin Case for Apple iPhone 14 Pro Max (6.7\" Inch) Adventurer Pro Camshield Camera Slider Military Rugged Grade Finish TPU + PC Tough Red\n",
      "10. Apple iPhone 14 Pro (256 GB) - Space Black\n",
      "11. Nillkin for iPhone 14 Pro Max Case with Sliding Camera Cover,[Full Around Protection],[Anti-Fingerprint],[Carbon Fiber Texture Anti-Scratch],Slim Shockproof Protective 6.7\",Deep Purple(Polycarbonate)\n",
      "12. Apple iPhone 14 Pro (512 GB) - Silver\n",
      "13. Apple iPhone 14 Pro (256 GB) - Gold\n",
      "14. Apple iPhone 14 Pro (128 GB) - Gold\n",
      "15. Apple iPhone 14 Pro (128 GB) - Space Black\n",
      "16. Apple iPhone 14 Pro (128 GB) - Silver\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_products(product_name):\n",
    "    # URL for Amazon's search results page with the user-provided product name\n",
    "    url = f\"https://www.amazon.in/s?k={product_name}\"\n",
    "\n",
    "    # Sending an HTTP GET request to the Amazon search page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parsing the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extracting product names from the search results\n",
    "    product_names = []\n",
    "    for product in soup.find_all(\"span\", class_=\"a-text-normal\"):\n",
    "        product_names.append(product.text)\n",
    "\n",
    "    return product_names\n",
    "\n",
    "# Taking user input for the product to search\n",
    "product_to_search = input(\"Enter the product to search on Amazon: \")\n",
    "\n",
    "# Calling the function to search for the products and printing the results\n",
    "products = search_amazon_products(product_to_search)\n",
    "print(\"Search Results:\")\n",
    "for idx, product in enumerate(products, start=1):\n",
    "    print(f\"{idx}. {product}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ffeb0",
   "metadata": {},
   "source": [
    "# 2.In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e762458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search on Amazon: naturaltein whey protein\n",
      "Data has been scraped and saved to 'amazon_products.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_amazon_products(product_name, max_pages=3):\n",
    "    product_data = []\n",
    "\n",
    "    # Iterate through the specified number of pages or until no more results are available\n",
    "    for page_number in range(1, max_pages + 1):\n",
    "        url = f\"https://www.amazon.in/s?k={product_name}&page={page_number}\"\n",
    "\n",
    "        # Sending an HTTP GET request to the Amazon search page\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Parsing the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extracting product details from the search results\n",
    "        products = soup.find_all(\"div\", class_=\"s-include-content-margin\")\n",
    "        for product in products:\n",
    "            name = product.find(\"span\", class_=\"a-text-normal\").text.strip()\n",
    "            url = \"https://www.amazon.in\" + product.find(\"a\", class_=\"a-link-normal\")[\"href\"]\n",
    "            brand = product.find(\"span\", class_=\"a-size-base-plus a-color-base\").text.strip()\n",
    "            price = product.find(\"span\", class_=\"a-price-whole\").text.strip()\n",
    "            return_exchange = product.find(\"div\", class_=\"a-icon-return-policy\") or {\"span\": {\"class\": \"-NA\"}}\n",
    "            return_exchange = return_exchange.text.strip() if return_exchange != {\"span\": {\"class\": \"-NA\"}} else \"-\"\n",
    "            expected_delivery = product.find(\"span\", class_=\"a-text-bold\").text.strip()\n",
    "            availability = product.find(\"span\", class_=\"a-size-base\").text.strip()\n",
    "\n",
    "            # Adding product details to the list\n",
    "            product_data.append({\n",
    "                \"Brand Name\": brand,\n",
    "                \"Name of the Product\": name,\n",
    "                \"Price\": price,\n",
    "                \"Return/Exchange\": return_exchange,\n",
    "                \"Expected Delivery\": expected_delivery,\n",
    "                \"Availability\": availability,\n",
    "                \"Product URL\": url\n",
    "            })\n",
    "\n",
    "    # Creating a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(product_data)\n",
    "\n",
    "    # Replacing missing values with \"-\"\n",
    "    df.fillna(\"-\", inplace=True)\n",
    "\n",
    "    # Saving the data to a CSV file\n",
    "    df.to_csv(\"amazon_products.csv\", index=False)\n",
    "    print(\"Data has been scraped and saved to 'amazon_products.csv'.\")\n",
    "\n",
    "# Taking user input for the product to search\n",
    "product_to_search = input(\"Enter the product to search on Amazon: \")\n",
    "\n",
    "# Calling the function to scrape and save the product details\n",
    "scrape_amazon_products(product_to_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c272fa95",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47955b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.13.0-py3-none-any.whl (9.5 MB)\n",
      "     ---------------------------------------- 9.5/9.5 MB 5.4 MB/s eta 0:00:00\n",
      "Collecting chromedriver-autoinstaller\n",
      "  Downloading chromedriver_autoinstaller-0.6.2-py3-none-any.whl (7.4 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
      "     -------------------------------------- 400.2/400.2 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: outcome, h11, exceptiongroup, chromedriver-autoinstaller, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed chromedriver-autoinstaller-0.6.2 exceptiongroup-1.1.3 h11-0.14.0 outcome-1.2.0 selenium-4.13.0 trio-0.22.2 trio-websocket-0.11.1 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium chromedriver-autoinstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3572b053",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_elements_by_css_selector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Fetch images for each search keyword\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords:\n\u001b[1;32m---> 61\u001b[0m     image_urls \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_image_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_links_to_fetch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     image_urls_dict[keyword] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image_urls)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Close the browser\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m, in \u001b[0;36mfetch_image_urls\u001b[1;34m(keyword, max_links_to_fetch)\u001b[0m\n\u001b[0;32m     20\u001b[0m scroll_to_end(browser)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Get all image thumbnail results\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m thumbnail_results \u001b[38;5;241m=\u001b[39m \u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements_by_css_selector\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg.Q4LuWd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m num_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(thumbnail_results)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m thumbnail_results[results_start:num_results]:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_elements_by_css_selector'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fcf4fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching images for 'fruits'...\n",
      "Failed to download image /images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif: Invalid URL '/images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif': No scheme supplied. Perhaps you meant http:///images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif?\n",
      "Downloaded fruits/image2.jpg\n",
      "Downloaded fruits/image3.jpg\n",
      "Downloaded fruits/image4.jpg\n",
      "Downloaded fruits/image5.jpg\n",
      "Downloaded fruits/image6.jpg\n",
      "Downloaded fruits/image7.jpg\n",
      "Downloaded fruits/image8.jpg\n",
      "Downloaded fruits/image9.jpg\n",
      "Downloaded fruits/image10.jpg\n",
      "Fetching images for 'cars'...\n",
      "Failed to download image /images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif: Invalid URL '/images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif': No scheme supplied. Perhaps you meant http:///images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif?\n",
      "Downloaded cars/image2.jpg\n",
      "Downloaded cars/image3.jpg\n",
      "Downloaded cars/image4.jpg\n",
      "Downloaded cars/image5.jpg\n",
      "Downloaded cars/image6.jpg\n",
      "Downloaded cars/image7.jpg\n",
      "Downloaded cars/image8.jpg\n",
      "Downloaded cars/image9.jpg\n",
      "Downloaded cars/image10.jpg\n",
      "Fetching images for 'Machine Learning'...\n",
      "Failed to download image /images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif: Invalid URL '/images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif': No scheme supplied. Perhaps you meant http:///images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif?\n",
      "Downloaded Machine Learning/image2.jpg\n",
      "Downloaded Machine Learning/image3.jpg\n",
      "Downloaded Machine Learning/image4.jpg\n",
      "Downloaded Machine Learning/image5.jpg\n",
      "Downloaded Machine Learning/image6.jpg\n",
      "Downloaded Machine Learning/image7.jpg\n",
      "Downloaded Machine Learning/image8.jpg\n",
      "Downloaded Machine Learning/image9.jpg\n",
      "Downloaded Machine Learning/image10.jpg\n",
      "Fetching images for 'Guitar'...\n",
      "Failed to download image /images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif: Invalid URL '/images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif': No scheme supplied. Perhaps you meant http:///images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif?\n",
      "Downloaded Guitar/image2.jpg\n",
      "Downloaded Guitar/image3.jpg\n",
      "Downloaded Guitar/image4.jpg\n",
      "Downloaded Guitar/image5.jpg\n",
      "Downloaded Guitar/image6.jpg\n",
      "Downloaded Guitar/image7.jpg\n",
      "Downloaded Guitar/image8.jpg\n",
      "Downloaded Guitar/image9.jpg\n",
      "Downloaded Guitar/image10.jpg\n",
      "Fetching images for 'Cakes'...\n",
      "Failed to download image /images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif: Invalid URL '/images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif': No scheme supplied. Perhaps you meant http:///images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif?\n",
      "Downloaded Cakes/image2.jpg\n",
      "Downloaded Cakes/image3.jpg\n",
      "Downloaded Cakes/image4.jpg\n",
      "Downloaded Cakes/image5.jpg\n",
      "Downloaded Cakes/image6.jpg\n",
      "Downloaded Cakes/image7.jpg\n",
      "Downloaded Cakes/image8.jpg\n",
      "Downloaded Cakes/image9.jpg\n",
      "Downloaded Cakes/image10.jpg\n",
      "Image download complete.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# List of search keywords\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# Function to fetch image URLs\n",
    "def fetch_image_urls(keyword, num_images=10):\n",
    "    image_urls = []\n",
    "    search_url = f\"https://www.google.com/search?q={keyword}&source=lnms&tbm=isch\"\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "\n",
    "    for img in img_tags[:num_images]:\n",
    "        image_urls.append(img.get('src'))\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "# Function to download images\n",
    "def download_images(image_urls, keyword):\n",
    "    os.makedirs(keyword, exist_ok=True)\n",
    "    for i, img_url in enumerate(image_urls):\n",
    "        try:\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(f\"{keyword}/image{i+1}.jpg\", 'wb') as handler:\n",
    "                handler.write(img_data)\n",
    "            print(f\"Downloaded {keyword}/image{i+1}.jpg\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download image {img_url}: {str(e)}\")\n",
    "\n",
    "# Fetch and download images for each keyword\n",
    "for keyword in keywords:\n",
    "    print(f\"Fetching images for '{keyword}'...\")\n",
    "    image_urls = fetch_image_urls(keyword, num_images=10)\n",
    "    download_images(image_urls, keyword)\n",
    "\n",
    "print(\"Image download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ed27d",
   "metadata": {},
   "source": [
    "# 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.comand scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d378c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\sonus\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\sonus\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sonus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920dbff0",
   "metadata": {},
   "source": [
    "# import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch smartphone details from Flipkart\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    url = f\"https://www.flipkart.com/search?q={search_query}&page=1\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = []\n",
    "    for product in soup.find_all('div', class_='_1AtVbE'):\n",
    "        try:\n",
    "            brand = product.find('div', class_='_4rR01T').text\n",
    "            name = product.find('a', class_='IRpwTa').text\n",
    "            color = product.find('a', class_='IRpwTa').text\n",
    "            specs = product.find_all('li', class_='rgWa7D')\n",
    "\n",
    "            ram, rom, primary_camera, secondary_camera, display_size, battery_capacity, price = [\"-\"] * 7\n",
    "\n",
    "            for spec in specs:\n",
    "                text = spec.text.lower()\n",
    "                if 'ram' in text:\n",
    "                    ram = spec.text.split()[0]\n",
    "                elif 'storage' in text:\n",
    "                    rom = spec.text.split()[0]\n",
    "                elif 'mp' in text and 'primary' in text:\n",
    "                    primary_camera = spec.text\n",
    "                elif 'mp' in text and 'secondary' in text:\n",
    "                    secondary_camera = spec.text\n",
    "                elif 'inch' in text:\n",
    "                    display_size = spec.text\n",
    "                elif 'mah' in text:\n",
    "                    battery_capacity = spec.text\n",
    "\n",
    "            product_url = \"https://www.flipkart.com\" + product.find('a', class_='IRpwTa')['href']\n",
    "\n",
    "            products.append({\n",
    "                \"Brand Name\": brand,\n",
    "                \"Smartphone Name\": name,\n",
    "                \"Color\": color,\n",
    "                \"RAM\": ram,\n",
    "                \"Storage(ROM)\": rom,\n",
    "                \"Primary Camera\": primary_camera,\n",
    "                \"Secondary Camera\": secondary_camera,\n",
    "                \"Display Size\": display_size,\n",
    "                \"Battery Capacity\": battery_capacity,\n",
    "                \"Price\": price,\n",
    "                \"Product URL\": product_url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return products\n",
    "\n",
    "# Search query for smartphones\n",
    "search_query = input(\"Enter smartphone name to search: \")\n",
    "\n",
    "# Scrape smartphone details and save in a DataFrame\n",
    "smartphones_data = scrape_flipkart_smartphones(search_query)\n",
    "df = pd.DataFrame(smartphones_data)\n",
    "\n",
    "# Save data to CSV\n",
    "df.to_csv(\"flipkart_smartphones.csv\", index=False)\n",
    "print(\"Data has been scraped and saved to 'flipkart_smartphones.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62c990",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1638aa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading geopy-2.4.0-py3-none-any.whl (125 kB)\n",
      "     -------------------------------------- 125.4/125.4 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting geographiclib<3,>=1.52\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 40.3/40.3 kB ? eta 0:00:00\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53390cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter city name: patna\n",
      "Latitude: 25.6093239, Longitude: 85.1235252\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    geolocator = Nominatim(user_agent=\"geo_scraper\")\n",
    "    location = geolocator.geocode(city_name)\n",
    "    \n",
    "    if location:\n",
    "        latitude, longitude = location.latitude, location.longitude\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Input: City Name\n",
    "city_name = input(\"Enter city name: \")\n",
    "\n",
    "coordinates = get_coordinates(city_name)\n",
    "if coordinates:\n",
    "    latitude, longitude = coordinates\n",
    "    print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
    "else:\n",
    "    print(\"Invalid city name or coordinates not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390f67c",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "727985bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved to 'gaming_laptops_digit.csv'.\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch gaming laptop details from digit.in\n",
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    laptops = []\n",
    "    for laptop in soup.find_all('div', class_='right-container'):\n",
    "        name = laptop.find('div', class_='TopNumbeHeading active sticky-footer').text.strip()\n",
    "        specs = laptop.find('div', class_='Specs-Wrap').text.strip()\n",
    "        price = laptop.find('div', class_='Block-price').text.strip()\n",
    "\n",
    "        laptops.append({\n",
    "            \"Name\": name,\n",
    "            \"Specifications\": specs,\n",
    "            \"Price\": price\n",
    "        })\n",
    "\n",
    "    return laptops\n",
    "\n",
    "# Scrape gaming laptop details and save in a DataFrame\n",
    "gaming_laptops_data = scrape_gaming_laptops()\n",
    "df = pd.DataFrame(gaming_laptops_data)\n",
    "\n",
    "# Save data to CSV\n",
    "df.to_csv(\"gaming_laptops_digit.csv\", index=False)\n",
    "print(\"Data has been scraped and saved to 'gaming_laptops_digit.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97324c60",
   "metadata": {},
   "source": [
    "# 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59a6e3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved to 'forbes_billionaires.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch billionaire details from Forbes\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/list/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    billionaires = []\n",
    "    for row in soup.find_all('div', class_='table-row'):\n",
    "        rank = row.find('div', class_='rank').text.strip()\n",
    "        name = row.find('div', class_='personName').text.strip()\n",
    "        net_worth = row.find('div', class_='netWorth').text.strip()\n",
    "        age = row.find('div', class_='age').text.strip()\n",
    "        citizenship = row.find('div', class_='countryOfCitizenship').text.strip()\n",
    "        source = row.find('div', class_='source').text.strip()\n",
    "        industry = row.find('div', class_='category').text.strip()\n",
    "\n",
    "        billionaires.append({\n",
    "            \"Rank\": rank,\n",
    "            \"Name\": name,\n",
    "            \"Net Worth\": net_worth,\n",
    "            \"Age\": age,\n",
    "            \"Citizenship\": citizenship,\n",
    "            \"Source\": source,\n",
    "            \"Industry\": industry\n",
    "        })\n",
    "\n",
    "    return billionaires\n",
    "\n",
    "# Scrape billionaire details and save in a DataFrame\n",
    "billionaires_data = scrape_forbes_billionaires()\n",
    "df = pd.DataFrame(billionaires_data)\n",
    "\n",
    "# Save data to CSV\n",
    "df.to_csv(\"forbes_billionaires.csv\", index=False)\n",
    "print(\"Data has been scraped and saved to 'forbes_billionaires.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901fd3b8",
   "metadata": {},
   "source": [
    "# 8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e005ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not Solved this question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc87e909",
   "metadata": {},
   "source": [
    "# 9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8de23c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved to 'hostelworld_hostels.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape hostel data from Hostelworld\n",
    "def scrape_hostel_data():\n",
    "    url = \"https://www.hostelworld.com/hostels/London/England\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    hostels = []\n",
    "    hostel_cards = soup.find_all('div', class_='fabhostel-card')\n",
    "\n",
    "    for card in hostel_cards:\n",
    "        name = card.find('h2', class_='title-2').text.strip()\n",
    "        distance = card.find('span', class_='description').text.strip()\n",
    "        ratings = card.find('div', class_='rating rating-summary-container big').text.strip()\n",
    "        total_reviews = card.find('div', class_='reviews').text.strip()\n",
    "        overall_reviews = card.find('div', class_='keyword').text.strip()\n",
    "        privates_price = card.find('a', class_='prices').text.strip().replace('\\n', '').replace('From\\n', '')\n",
    "        dorms_price = card.find('a', class_='prices').find_next('a', class_='prices').text.strip().replace('\\n', '')\n",
    "\n",
    "        facilities = [facility.text.strip() for facility in card.find_all('i', class_='facility-icon')]\n",
    "\n",
    "        description = card.find('div', class_='description').text.strip().replace('\\n', '')\n",
    "\n",
    "        hostels.append({\n",
    "            \"Name\": name,\n",
    "            \"Distance\": distance,\n",
    "            \"Ratings\": ratings,\n",
    "            \"Total Reviews\": total_reviews,\n",
    "            \"Overall Reviews\": overall_reviews,\n",
    "            \"Privates From Price\": privates_price,\n",
    "            \"Dorms From Price\": dorms_price,\n",
    "            \"Facilities\": \", \".join(facilities),\n",
    "            \"Description\": description\n",
    "        })\n",
    "\n",
    "    return hostels\n",
    "\n",
    "# Scrape hostel data and save in a DataFrame\n",
    "hostel_data = scrape_hostel_data()\n",
    "df = pd.DataFrame(hostel_data)\n",
    "\n",
    "# Save data to CSV\n",
    "df.to_csv(\"hostelworld_hostels.csv\", index=False)\n",
    "print(\"Data has been scraped and saved to 'hostelworld_hostels.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bb2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
